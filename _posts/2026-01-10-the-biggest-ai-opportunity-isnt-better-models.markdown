---
layout: single
title: "The Biggest AI Opportunity Isn't Better Models"
excerpt: "There are specialists in every field who could massively accelerate their work with AI tools - but they don't know what's possible. I want to find them."
category: "AI Strategy & Leadership"
tags: ["ai-adoption", "domain-experts", "claude-code", "ai-tools", "strategy"]
---

Six months ago, an old colleague, Martin, reached out to me after reading some of my writing about AI. He's an expert in zeolites - microporous crystalline minerals used in catalysis and filtration. Only about 250 zeolite structures have been synthesized, but millions are theoretically possible. We sat down and he showed me his website, [Hypothetical Zeolites](http://www.hypotheticalzeolites.net/), and software he'd built to computationally generate and test potential new structures. People from industry were using it. He'd been at this for years. I was fascinated by this domain I had no idea existed - the next day I spent an hour on ChatGPT voice going back and forth learning more about zeolites. It made me realize just how many different things you can be an expert on in this world.

He had ideas for improving his platform, so I nudged him to start experimenting with AI tools. [Claude Code](/2025/06/14/ai-coding-tools-journey.html) existed but was still nascent - not yet widely adopted. He started using various AI tools and had some success with them. Now, six months later, thinking about what he wanted to build - ways to make his research faster - I keep imagining what would happen if he got proficient with today's tools. Really proficient, not just dabbling. He might be able to move 10x faster on problems he's been chipping away at for years.

That got me thinking. How many other people like Martin are out there? By "like Martin" I mean: deep expertise in a narrow field, probably building tools or workflows to support their work, but not deep in the rabbit hole of understanding the best ways to use leading edge AI tools.

Here's what happens when someone like that figures it out. Andrew Hall, a political economist at Stanford, recently [shared on Twitter](https://x.com/ahall_research/status/2007603340939800664) how he used Claude Code to replicate and extend one of his old papers on vote-by-mail and election turnout. It downloaded his original repo, translated Stata code to Python, crawled the web for updated election and census data, ran new analyses through 2024, created tables and figures, performed a lit review, wrote a new paper, and pushed everything to GitHub. The whole thing took about an hour. His take: "This is an insane paradigm shift in how empirical work is done." What he did is impressive - and once you know what these tools can do, it's not surprising at all.

## My Hypothesis

Martin's work is unique, but this pattern isn't. There are specialists in every field - rare diseases, educational methods, obscure industrial processes. They've built tools and workflows for years. Many are using AI tools, but there's a gap between that and knowing what the leading edge can actually do.

AI coding tools can now write working software from a description of what you want. That's what Andrew Hall did - and it's what Martin could be doing too.

The tools exist. The experts exist. But the experts don't know what the tools can do, and the people who know the tools don't understand what the experts are working on.

## If This Is True, What Do You Do About It?

So how do you fix this? My first thought was scale - build a platform, write guides, create content that reaches lots of people.

But I don't think that's how this works. You need to show people the capabilities in the context of what they're already doing - that's what creates the "aha" moment. A generic guide or tutorial just can't do that.

I keep hearing this narrative: if AI progress stopped today, we'd have 10 years of adoption work ahead of us just to integrate what already exists. I actually think that's true based on what I've seen. It's frustrating - we have these superpowers available and it's going to take a while for most people to use them. That's probably always going to be true. But is there a way to shortcut this for people like Martin?

I think there is. Find people like Martin - people doing work where I look at it and think "I have no idea what this is, but it seems important." People where I can see a pattern for how they could use these tools more effectively. Sit down with them, understand what they're trying to do, and figure out what's possible.

## Two Asks

**If you're a specialist doing deep work in a narrow field:** I want to hear what you're working on. What's tedious? What would you build if you could? I'm not selling anything - I just want to understand where you're stuck and show you what might be possible.

**If you're already deep in AI like me:** Think about finding a Martin in your network. The most useful thing you can do right now probably isn't building another demo. It's sitting down with someone who knows their field cold and showing them what these tools can do for their specific work.

If this sounds like you, you have a view on this hypothesis, you know someone like Martin, or you know someone who's already in the weeds using these tools to accelerate their work - send me a message on [LinkedIn](https://www.linkedin.com/in/mattstockton/) or shoot me an email. I'd love to hear more examples.
